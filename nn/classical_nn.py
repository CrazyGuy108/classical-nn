from create_midi import create_midi
import keras
from keras.activations import relu, sigmoid, tanh
from keras.layers import Dense, Dropout, Input, LSTM, TimeDistributed
from keras.losses import mean_squared_error
from keras.models import Model, Sequential
from keras.optimizers import RMSprop, SGD
from mid import int_to_note, n_vocab, real_d_in
import numpy as np

noise_length = 100
note_data_length = 3

g_input = Input(shape=(None, noise_length))
lstm1 = LSTM(units=100, dropout=0.4, return_sequences=True)(g_input)
lstm2 = LSTM(units=50, dropout=0.3, return_sequences=True)(lstm1)
g_output = Dense(units=note_data_length, activation="sigmoid",
        name="g_output")(lstm1)
gm = Model(inputs=g_input, outputs=g_output)
gm.compile(optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True),
        loss=mean_squared_error)

d_input = Input(shape=(None, note_data_length))
lstm1 = LSTM(units=100, dropout=0.3)(d_input)
d_output = Dense(units=1, activation="sigmoid", name="d_output")(lstm1)
dm = Model(inputs=d_input, outputs=d_output)
dm.compile(optimizer=RMSprop(lr=0.1, clipvalue=1.0, decay=6e-8),
        loss=mean_squared_error)

am = Sequential()
am.add(gm)
am.add(dm)
am.compile(optimizer=RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8),
        loss=keras.losses.mean_squared_error, metrics=["accuracy"])

def get_noise_vector():
    return np.random.uniform(size=(1, noise_length))

# generates an amount of notes using the generator
def generate(g_notes=None):
    if g_notes is None:
        g_notes = np.random.randint(100, 1000)
    g_in = np.random.uniform(size=(1, g_notes, noise_length))
    return gm.predict(g_in, batch_size=1)

# plays a new midi file generated by the generator model
def play_generated(outfile=None):
    generated_song = generate()[0]
    processed_song = []
    for note_data in generated_song:
        processed_song.append([int_to_note[int(note_data[0] * n_vocab)],
                    note_data[1], note_data[2]])
    create_midi(processed_song, outfile)

# trains the discriminator network for one full epoch
def train_discriminator():
    # expected output for each type song
    real = np.zeros(shape=(1, 1))
    fake = np.ones(shape=(1, 1))

    # generate training data
    d_in = []
    d_out = []
    for i in range(len(real_d_in)):
        d_in.append(real_d_in[i])
        d_out.append(np.array(real))
        d_in.append(generate().reshape((1, -1, 3)))
        d_out.append(np.array(fake))

    # train one full epoch with all the training data
    for x, y in zip(d_in, d_out):
        dm.train_on_batch(x, y)

# trains the adversarial (generator+discriminator) network for one full batch
def train_adversarial():
    g_notes = np.random.randint(100, 1000)
    noise_vector = get_noise_vector()
    a_in = np.full(shape=(1, g_notes, noise_length), fill_value=noise_vector)
    a_out = np.ones(shape=(1, 1))
    am.train_on_batch(a_in, a_out)
